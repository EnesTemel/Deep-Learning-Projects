{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Importing libraries \r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "import time \r\n",
    "import datetime\r\n",
    "import smtplib"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Connecting to Website and Pulling in data\r\n",
    "URL=\"https://www.amazon.com/Python-Data-Science-Handbook-Essential/dp/1491912057/ref=sr_1_1?dchild=1&keywords=python+for+data+science&qid=1630352873&sr=8-1\"\r\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\r\n",
    "page = requests.get(URL, headers=headers)\r\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\r\n",
    "title= soup2.find(id=\"productTitle\").get_text()\r\n",
    "price= soup2.find(id=\"newBuyBoxPrice\").get_text()\r\n",
    "print(title)\r\n",
    "print(price)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "          Python Data Science Handbook: Essential Tools for Working with Data\n",
      "         \n",
      "\n",
      "                               $35.00\n",
      "                              \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Cleaning up the data a little bit\r\n",
    "title=title.strip()\r\n",
    "price=price.strip()[1:] \r\n",
    "print(title)\r\n",
    "print(price)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python Data Science Handbook: Essential Tools for Working with Data\n",
      "35.00\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Creating a CSV and writing the headers and the data into the file\r\n",
    "import csv \r\n",
    "header =[\"Title\",\"Price\",\"Date\"]\r\n",
    "data=[title,price,today]\r\n",
    "#with open('AmazonWebScraperDataSet.csv','w',newline='',encoding='UTF8')as gg:\r\n",
    "#    writer = csv.writer(gg)\r\n",
    " #   writer.writerow(header)\r\n",
    "  #  writer.writerow(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Creating Timestamp for the output to track when data was collected\r\n",
    "import datetime\r\n",
    "today=datetime.date.today()\r\n",
    "print(today)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-08-31\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#For checking csv purposes\r\n",
    "import pandas as pd \r\n",
    "df=pd.read_csv(r\"C:\\Users\\lenvo\\Desktop\\Pyhton\\AmazonWebScraperDataSet.csv\")\r\n",
    "print(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                               Title  Price        Date\n",
      "0  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "1  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "2  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "3  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "4  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Now we are appending data to our csv\r\n",
    "with open('AmazonWebScraperDataSet.csv','a+',newline='',encoding='UTF8')as gg:\r\n",
    "    writer = csv.writer(gg)\r\n",
    "    writer.writerow(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def check_price():\r\n",
    "    URL=\"https://www.amazon.com/Python-Data-Science-Handbook-Essential/dp/1491912057/ref=sr_1_1?dchild=1&keywords=python+for+data+science&qid=1630352873&sr=8-1\"\r\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\r\n",
    "    page = requests.get(URL, headers=headers)\r\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")\r\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\r\n",
    "    title= soup2.find(id=\"productTitle\").get_text()\r\n",
    "    price= soup2.find(id=\"newBuyBoxPrice\").get_text()\r\n",
    "    title=title.strip()\r\n",
    "    price=price.strip()[1:] \r\n",
    "    price=int(price)\r\n",
    "    import datetime\r\n",
    "    today=datetime.date.today()\r\n",
    "    print(today)\r\n",
    "    import csv \r\n",
    "    header =[\"Title\",\"Price\",\"Date\"]\r\n",
    "    data=[title,price,today]\r\n",
    "    with open('AmazonWebScraperDataSet.csv','a+',newline='',encoding='UTF8')as gg:\r\n",
    "        writer = csv.writer(gg)\r\n",
    "        writer.writerow(data)\r\n",
    "    if(price < 20):\r\n",
    "        send_mail()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while(True):\r\n",
    "    check_price()\r\n",
    "    time.sleep(86400)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12252/464025585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcheck_price\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m86400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12252/2083439844.py\u001b[0m in \u001b[0;36mcheck_price\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mURL\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"https://www.amazon.com/Python-Data-Science-Handbook-Essential/dp/1491912057/ref=sr_1_1?dchild=1&keywords=python+for+data+science&qid=1630352873&sr=8-1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Accept-Encoding\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"gzip, deflate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Accept\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DNT\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Connection\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"close\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Upgrade-Insecure-Requests\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msoup1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msoup2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#For checking csv purposes\r\n",
    "import pandas as pd \r\n",
    "df=pd.read_csv(r\"C:\\Users\\lenvo\\Desktop\\Pyhton\\AmazonWebScraperDataSet.csv\")\r\n",
    "print(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                               Title  Price        Date\n",
      "0  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "1  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "2  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "3  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n",
      "4  Python Data Science Handbook: Essential Tools ...   35.0  2021-08-30\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Send e-mail when price drops under 20$\r\n",
    "def send_mail():\r\n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com',465)\r\n",
    "    server.ehlo()\r\n",
    "    #server.starttls()\r\n",
    "    server.ehlo()\r\n",
    "    server.login('enesmaliktemel@gmail.com','*************')\r\n",
    "    \r\n",
    "    subject = \"The book you want is below $20! Now is your chance to buy!\"\r\n",
    "    body = \"Enes, This is the moment we have been waiting for. Now is your chance to pick up the book of your dreams. Don't mess it up! Link here:https://www.amazon.com/Python-Data-Science-Handbook-Essential/dp/1491912057/ref=sr_1_1?dchild=1&keywords=python+for+data+science&qid=1630352873&sr=8-1\" \r\n",
    "   \r\n",
    "    msg = f\"Subject: {subject}\\n\\n{body}\"\r\n",
    "    \r\n",
    "    server.sendmail(\r\n",
    "        'enesmaliktemel@gmail.com',\r\n",
    "        msg\r\n",
    "     \r\n",
    "    )\r\n",
    "send_mail()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}